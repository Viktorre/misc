{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "Epoch 1/3\n",
      "938/938 [==============================] - 14s 13ms/step - loss: 0.3587 - accuracy: 0.8967 - val_loss: 0.2100 - val_accuracy: 0.9366\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 13s 13ms/step - loss: 0.1494 - accuracy: 0.9542 - val_loss: 0.1473 - val_accuracy: 0.9549\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0963 - accuracy: 0.9717 - val_loss: 0.1330 - val_accuracy: 0.9602\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1259 - accuracy: 0.9613\n",
      "0.12588179111480713\n",
      "0.9613000154495239\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()  \n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.5, random_state=42) #valid data for hyper params tuning\n",
    "\n",
    "\n",
    "def return_compiled_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #model.add(tf.keras.layers.Flatten())   #Flatten the images! Could be done with numpy reshape\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28))),\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    # model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu, input_shape= x_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))   #10 because dataset is numbers from 0 - 9\n",
    "    model.compile(optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = return_compiled_model()\n",
    "#model.summary()\n",
    "# log_dir = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_name = 'main_run_'\n",
    "log_dir = \"logs/fit/\" + log_name + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "with tf.device('/CPU:0'): # it can be with '/CPU:0'\n",
    "# with tf.device('/GPU:0'): # comment the previous line and uncomment this line to train with a GPU, if available.\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        # batch_size = batch_size,\n",
    "        epochs=3, \n",
    "        # verbose=1,\n",
    "        shuffle=True,\n",
    "        # steps_per_epoch = int(normed_train_data.shape[0] / batch_size) ,\n",
    "        validation_data = (x_valid, y_valid), \n",
    "        callbacks=[tensorboard_callback]  \n",
    "    )\n",
    "\n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)  # evaluate the out of sample data with model\n",
    "print(val_loss)  # model's loss (error)\n",
    "print(val_acc)  # model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [01:46<21:12, 106.07s/it]"
     ]
    }
   ],
   "source": [
    "#import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np # for math and arrays\n",
    "\n",
    "from DataHandling.DataImporter import DataImporter\n",
    "\n",
    "all_ds = pd.read_csv('iris_dataset.csv')\n",
    "\n",
    "all_ds = all_ds.sample(frac=1) # This will randomly shuffle the rows to make sure the data is not sorted\n",
    "\n",
    "# Split the data into 60% train and 40% test (later will divide the test to test and validate.)\n",
    "train_dataset, temp_test_dataset =  train_test_split(all_ds, test_size=0.2)\n",
    "# Split the test_dataset dataframe to 50% test and 50% validation. [this will divide the dataset into 60% train, 20% validate, and 20% test]\n",
    "test_dataset, valid_dataset =  train_test_split(temp_test_dataset, test_size=0.5)\n",
    "\n",
    "train_labels1 = train_dataset.pop('target')\n",
    "test_labels1 = test_dataset.pop('target')\n",
    "valid_labels1 = valid_dataset.pop('target')\n",
    "\n",
    "# Encode the labeles\n",
    "train_labels = pd.get_dummies(train_labels1, prefix='Label')\n",
    "valid_labels = pd.get_dummies(valid_labels1, prefix='Label')\n",
    "test_labels = pd.get_dummies(test_labels1, prefix='Label')\n",
    "\n",
    "# Plot the relationship between each two variables to spot anything incorrect.\n",
    "#train_stats = train_dataset.describe()\n",
    "#sns.pairplot(train_stats[train_stats.columns], diag_kind=\"kde\") # or diag_kind='reg'\n",
    "\n",
    "# Statistics on the train dataset to make sure it is in a good shape. (you may display the same stat for test and validate)\n",
    "train_stats = train_dataset.describe()\n",
    "#train_stats.pop(\"target\")\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "normed_train_data = pd.DataFrame(StandardScaler().fit_transform(train_dataset), columns=train_dataset.columns, index=train_dataset.index)\n",
    "normed_test_data = pd.DataFrame(StandardScaler().fit_transform(test_dataset), columns=test_dataset.columns, index=test_dataset.index)\n",
    "normed_valid_data = pd.DataFrame(StandardScaler().fit_transform(valid_dataset), columns=valid_dataset.columns, index=valid_dataset.index)\n",
    "    \n",
    "\n",
    "def return_compiled_model(how_many_dense_layers:int, layer_size:int) ->tf.keras.models.Sequential:\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten()),\n",
    "    for i in range(how_many_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(layer_size, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))   \n",
    "    model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model_short_name(how_many_dense_layers:int, layer_size:int) ->str:\n",
    "    return str(how_many_dense_layers)+'layers_'+str(layer_size)+'nodes_'\n",
    "\n",
    "all_model_specs = {\"layers\": 0 ,\"nodes\": 32 }, {\"layers\": 1 ,\"nodes\": 32 }, {\"layers\": 1 ,\"nodes\": 64 }, {\"layers\": 1 ,\"nodes\": 128 }, {\"layers\": 1 ,\"nodes\": 256 }, {\"layers\": 2 ,\"nodes\": 32 }, {\"layers\": 2 ,\"nodes\": 64 }, {\"layers\": 2 ,\"nodes\": 128 }, {\"layers\": 2 ,\"nodes\": 256 }, {\"layers\": 3 ,\"nodes\": 32 }, {\"layers\": 3 ,\"nodes\": 64 }, {\"layers\": 3 ,\"nodes\": 128 }, {\"layers\": 3 ,\"nodes\": 256 }\n",
    "\n",
    "for model_spec in tqdm(all_model_specs):\n",
    "    log_name = create_model_short_name(model_spec['layers'],model_spec['nodes'])\n",
    "    log_dir = \"logs_iris/fit/\" + log_name + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model = return_compiled_model(model_spec['layers'],model_spec['nodes'])\n",
    "    with tf.device('/CPU:0'): # it can be with '/CPU:0'\n",
    "        history = model.fit(x_train, y_train, epochs=20, shuffle=True, validation_data = (x_valid,y_valid), verbose=0, callbacks=[tensorboard_callback] )\n",
    "\n",
    "# 1. das nochmal laufen lassen\n",
    "# 2. das gleiche für iris\n",
    "# 3. das gleiche für titatnic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7acfbe3706fe0fa8597ca3088a4f4c75dd0f746ca2f43477b557a1281a0f5f04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvsentdex': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
